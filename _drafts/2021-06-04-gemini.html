--- 
title: 
layout: post 
date: 2024-08-15 23:50 
tag: 
- Chicago 
- tech 
headerImage: false
category: projects
description: "experimenting with vertexAI and gemini"
rel-path:  /assets/images/projects/gemini-taxi 
author: kjros 
--- 

<p>
    In all four years at college, I refused to use ChatGPT to write code or essays, unlike many of my peers. 
    I wanted to build a foundation of knowledge, and prided myself on personal creativity and growth. 
</p>

<p>
    Thus, being thrown into a world of generative AI, RAG, and prompt engineering soon into my first job came at quite a shock. 
</p>

<h3>Background</h3>

<p>
    <a href="https://www.linkedin.com/company/66degrees/posts/?feedView=all">66degrees</a> is a Google Cloud Partner organization, specializing 
    in technology and solutions consulting. 
    After graduation, I joined as a specialist transitioning to the AI/ML team come the end of the summer. 
    I had interned last summer, but this year had a massive focus on all things data science, machine learning, and genAI. 
</p>

<p>
    Along with onboarding, shadowing, and studying for Google's Professional Machine Learning Engineer Certification (I passed, by the way), 
    we were tasked with a small capstone project. 
    It served as a kickstart to one of the required projects tasked to 66degrees by Google, as way of keeping their Partner status.
    For us, it was a six week end-to-end engagement, and a way for us to experiment with the Google Cloud Platform (GCP) and 
    <a href="https://cloud.google.com/vertex-ai?hl=en">Vertex AI</a>, their AI development platform. 
</p>

<p>
    The public dataset was historical data on Chicago taxi rides, <a href="https://data.cityofchicago.org/Transportation/Taxi-Trips-2013-2023-/wrvz-psew/about_data" target="_blank">
        published by the city</a>.
    We were to build an ML model to predict fare based on a trip's information, similar to modern apps like Uber or Lyft. 
</p>

<h3>Implementation</h3>

 <p>
    Exporatory analysis was conducted in <a href="https://cloud.google.com/vertex-ai-notebooks/?hl=en" target="_blank">Vertex AI Workbench</a>, 
    where we found the most important features included distance, time, and whether or not trips were to/from Midway or O'Hare. 
    Fares didn't vary significantly by time of day, day of the week, or month. 
    Finally, when running simple regression models using scikit-learn, we found shuffling the data at random vs
    ordering by time achieved similar performance. Thus, this was not necessarily a time-series forecasting problem. 

<p>
    Our next step was training and testing. We used <a href="https://cloud.google.com/automl?hl=en" target="_blank">AutoML</a> 
    for Regression in Vertex AI, a no-code ML solution that would choose the best model and hyperparameters for our task. 
    With minimal feature engineering, we achieved an R-score of 0.89. 
 </p>

 <p>
    Having a well-performing, simple model, we moved to creating an end-to-end pipeline. 
    This was done using the <a href="https://www.kubeflow.org/docs/components/pipelines/" href="_target">Kubeflow Pipelines</a>, 
    a platform for building scalable workflows with Docker. 
    Modularity is achieved through pipeline components, and we also utilized several pre-built 
    <a href="https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.16.1/" href="_target">
        Google Cloud Pipeline Components
        </a> for tasks such as uploading and serving the model to Vertex AI. 
 </p>

 <p>
    Our final pipeline ingested data from BigQuery, performed our feature engineering script, uploaded the new data to Vertex AI, 
    and ran AutoML with a standard training, testing, and validation data split. Upon a high enough R-score, 
    the model would be deployed to a pre-existing endpoint and serve all future prediction requests. We also configured 
    model monitoring jobs and version control for when a new model was uploaded. 
 </p>

 <p>
    As the primary conductor of all things Kubeflow, and having never dabbled in MLOps before, 
    this served as a good learning experience!
 </p>

 <p>
    For more interactivity, we used <a href="https://streamlit.io/" target="_blank">Streamlit</a>, an open-source Python 
    framework for quickly building data apps. Enter your source, destination, and time to leave; 
    get back your fare estimation and a fun little map of your route! 
 </p>

 <p>
    Would not recommend trying to go from Chicago to San Francisco, though. That might rack you a lot of charges. 
 </p>

 <h3>OK, but what about all that generative stuff you were talking about earlier?</h3>

 <p>
    Having achieved most of this well before the six-week mark, we were idle, restless, and interested in 
    experimenting with generative AI. 
 </p>

 <p>
    Thus, we began looking at implementing a chatbot our Streamlit app for answering user questions. 
    The main objective was to translate natural language queries into SQL, fetching the information from BigQuery, 
    and formatting the answer correctly. 
 </p>

 <p>
    On the back-end, we used <a href="https://deepmind.google/technologies/gemini/flash/" href="_target">Gemini 1.5 Flash</a>, 
    one of Google's foundation models.
    It is capable of processing text, images, and audio for multimodal tasks like classification or generation. 
 </p>

 <p>
    For the first pass, we supplied the data schema in the model's context, 
    including column names, data types, and descriptions. We then passed along the user question and asked it to translate into SQL. 
    Though this performed fairly well, we would occasionally run into formatting errors or depreciated methods with the BigQuery SQL syntax. 
 </p>

 <p>
    To alleviate this, we tried few-shot prompting: a prompt engineering technique for LLMs, wherein 
    examples of correct (and incorrect) input/outputs are given. This helped guide our model towards our desired output. 
 </p>

 <p>
    After the SQL query was constructed and executed, we asked the model to reformat the response in natural language, 
    and the result would be sent to the app. 
 </p>

 <p>
    After some experimentation with prompting and integration into our data app, we had built a simple chat functionality! 
    It could tell you the most popular days for rides, return the most expensive fares, and identify trends in the data. 
 </p>

 <p>
    Of course, we wanted to keep the app as intuitive and informative as possible, so we thought it would be fun 
    implementing graphing capabilities as well. We instructed the model to 
    search for phrases like "graph taxi trips by month", or "create a plot of fare distribution."
    After fetching the information, it would try to write Python code with <i>pandas</i> and <i>numpy</i> 
    for plotting the results. 
 </p>

 <p>
    Again, we supplied the model with a few examples of correct Python code. Upon successful code generation, 
    it would be passed to the Streamlit for rendering! 
 </p>

 <h3>Discussion</h3>

 <p>
    This project was just a glimpse into some of the work 66degrees does. 
    I am excited to learn more about generative AI and machine learning solutions. 
 </p>

 <p>
    However, a word of caution. Our use case was pretty simple: translating natural language into SQL back into natural language (and code!)
    had a limited scope and room for error. There wasn't any major consequence for incorrect results, other than perhaps 
    returning an angry error message in our app, or having an awkward moment when we presented to the AI/ML team. 
 </p>

 <p>
    LLM hallucinations (incorrect or misleading responses) pose a non-trivial risk: in this age, we do not want to 
    spread misinformation or biased and discriminatory content. There are  
    <a href="https://dr-arsanjani.medium.com/navigating-the-challenges-of-hallucinations-in-llm-applications-strategies-and-techniques-for-ab2b5ddc4a63"
    target="_blank">methods to mitigate these issues</a>, though we should always be cautious of what technologies we are using, 
    and how we are using them. 
 </p>