---
title: "Clustering and Segmentation Approaches for Big Data Filtering "
layout: post
date: 2022-12-24 01:14
tag: 
- research
- IIT
- Chicago
image: /assets/images/projects/kmeans-light.jpg
image-dark: /assets/images/projects/kmeans-dark.png
headerImage: true
category: projects
hidden: true 
description: "My research at SURE on data reduction and machine learning"
author: kjros
externalLink: false
---

<!-- <div class="text-center">
    Summer Undergraduate Research Experience (SURE)
    </h3>
    <p class="fs-4 fw-light">
    Chicago, IL
    </p>
    <p class="fs-5 fw-light">
    Summer 2021
    </p>
</div>
<h3> -->

<p>
<a href="https://www.iit.edu/computing/research/student-research/SURE"
    target="_blank">Summer Undergraduate Research Experience</a> (SURE)
    is a program held by IIT's College of Computing every summer. Undergraduates spend a few months working with professors and
    graduate students on research in computer science and applied mathematics. In 2021, I worked with Professor Lulu Kang 
    on her research in data filtering using unsupervised machine learning. 
</p>

<div class="text-center">
    <img src="/assets/images/projects/sure-siegal.jpeg" alt="SURE program at Siegal Hall" style="object-fit: contain;">
</div>

<p>  
    I examined prior methods in filtering information in large datasets, which have become more complex and harder to analyze 
    due to modern sensing systems. These methods frame data reduction as a minimization problem: minimizing 
    redundant information, while still maintaining the most important information. 
</p>

<div class="text-center">
    <img src="/assets/images/projects/sure-cardinality.png" alt="hub cardinality vs cluster graph" style="object-fit: contain;">
</div>

<p>  
    Building off of past research, I studied a data filtering method proposed by Professor Kang, 
    which relied on k-means clustering and sampling methods. 
    In general, this algorithm splits data into segments and optimally clusters each segment. It repeatedly 
    samples from each cluster, until the information loss falls below a given tolerance, and then reconstructs the newly 
    filtered dataset.
</p>

<div class="text-center">
    <img src="/assets/images/projects/sure-entropy-loss.png" alt="entropy loss vs data points graph" style="object-fit: contain">  
</div>

<p>  
    I coded this algorithm in MATLAB.
    For a case study, I used a dataset consisting of nearly 25,000 data entries and 32 response variables, 
    provided by a consumer goods corporation. 
    I experimented with different parameters, such as tolerance and optimal filtering ratios. Finally, I compared 
    the performances of this method and random sampling. 
</p>
<p>
    At the end of the seven weeks, I gave a presentation, "<em>Clustering and Segmented Approaches for Big Data Filtering</em>," 
    on my results to the SURE program. 
</p>